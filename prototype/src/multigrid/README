+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+
+ MGD3: PARALLEL 3-D MULTIGRID PACKAGE
+
+ Author: Bernard Bunner (bunner@engin.umich.edu)
+ January 1998
+
+ 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+
+  WHAT IT IS, WHAT IT DOES, AND SOME IMPLEMENTATION
+  FEATURES
+
mgd3 is a parallel 3-d multigrid program which solves the
non-separable Poisson equation:

d(cof(x,y,z)*d(phi)/dx)/dx+d(cof(x,y,z)*d(phi)/dy+
d(cof(x,y,z)*d(phi)/dz)/dz=rhs(x,y,z)

on a staggered grid. The rectangular domain has a constant
grid step in both directions and is decomposed into 
rectangular subdomains. In discretized form, this equation
can be written as

  [ cof(i+1/2,j,k)*(phi(i+1,j,k)-phi(i,j,k))
   -cof(i-1/2,j,k)*(phi(i,j,k)-phi(i-1,j,k)) ] / (dx*dx)
+ [ cof(i,j+1/2,k)*(phi(i,j+1,k)-phi(i,j,k))
   -cof(i,j-1/2,k)*(phi(i,j,k)-phi(i,j-1,k)) ] / (dy*dy)
+ [ cof(i,j,k+1/2)*(phi(i,j,k+1)-phi(i,j,k))
   -cof(i,j,k-1/2)*(phi(i,j,k)-phi(i,j,k-1)) ] / (dz*dz)
=  rhs(i,j,k)

Both periodic, Neumann (zero-derivative at the domain
boundary) and Dirichlet (constant value at the domain
boundary) are possible. Periodic BCs in the x-direction
mean:
    a(i+nxp2-2,j,k)=a(i,j,k) for all (i,j,k).
Since the grid is staggered, walls are located half-way
between the boundary phif nodes. The following figure 
illustrates this for the case of an horizontal wall:

    phi(i-1,j,k)     phi(i,j,k)      phi(i+1,j,k)
       x                x                x
   _____________________________________________

    phi(i-1,j,k)     phi(i,j,k)      phi(i+1,j,k)
       x                x                x

Between neighboring subdomains, messages are passed using 
the MPI message passing library. The subroutines gxch1pla,
gxch1lin, and gxch1cor are used to exchange respectively 
planes, lines and corners of boundary data. Both blocking
and non-blocking communications are possible; the user
chooses by setting compiler directive NBLOCKGR in the
compdir.inc file to either 1 (non-blocking comms) or 0
(blocking comms).

All files are written in Fortran 77 but have not been
written or tested to conform to any standard. Both single
and double precision are possible; again, the choice is
made by setting the double_precision compile directive in
the compdir.inc file to 0 (single precision) or 1 (double
precision).

The compdir.inc file contains a few other items. The use
of the WMGD compiler directive is described below. cdebug
sets whether the code should be compiled to include timing
information, for which a few variables are also declared.
This timing uses the MPI_WTIME wall-clock timer and works.
The numbering conventions may seem strange, but I did not
want to change them from the main code. The subroutine to
postprocess the timing information is not included. If
the value of any compiler directive is changed, the entire
code should be recompiled. For that, the user must manually
remove the *.f *.o *.a files and re-make.

I use the package in conjunction with a hydrodynamic flow
solver and run it almost exclusively on an IBM-SP2, for which
a Makefile is included. I have also checked once that it
ran on an HP-EXEMPLAR. The periodic and Neumann boundary
conditions have been tested extensively, but Dirichlet 
conditions have not.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+
+  WHAT IS THIS "WMGD" COMPILER DIRECTIVE?
+  THE RESTRICTION AND INTERPOLATION OPERATORS
+
mgd3 is really two different packages put together. I
originally developed it for periodic boundary conditions and
used a vertex-centered grid setup. This corresponds to the
WMGD compiler directive being set to 0.

  |------|-----|-----|-----|-----|            fine
  1      2     3     4     5     6 

  |------------|-----------|-----------|      coarse
  1            2           3           4

Restriction is done by full weighting, correction by bilinear
interpolation. For derivative boundary conditions, using this
grid setup would result in a loss of accuracy at the 
boundaries. More precisely, the numerical scheme would only
be first-order at the boundaries at all but the finest grid
level, whereas it would be second-order everywhere else. To
avoid that loss of accuracy, the option of an alternative,
cell-centered grid setup is offered. It corresponds to
WMGD=1. 

           |                       |
        |--|--|-----|-----|-----|--|--|       fine
        1  |  2     3     4     5  |  6
           |                       |
     |-----|-----|-----------|-----|-----|    coarse
     1     |     2           3     |     4
           |                       |
          wall                    wall

Restriction and correction are done by area weighting. This
option works for all types of boundary conditions. However,
with periodic boundary conditions, this option is slightly
slower than WMGD=0. In conclusion, if all boundary conditions
are periodic, use WMGD=0. If some are not, use WMGD=1.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+
+  HOW TO USE IT?
+
It is assumed that, prior to accessing the multigrid code,
the calling program has created a cartesian topology and
is able to provide mgd3 with:
- the indices sx,ex,sy,ey,sz,ez of the limits of the
  subdomain the process operates on;
- 4 communicators: comm3d, and comm3dp, comm3dl, comm3dc,
  which are simply copies of comm3d. They are used for
  safety reasons in order to isolate the communication
  worlds for the exchange of plane, line, and corner
  boundary data. 
- the rank of the process, myid;
- the ranks of the 26 neighbors, neighbor(26), and the
  boundary conditions at the boundaries with these neighbors,
  bd(26). The figure below shows the convention used to
  represent the topology.


 plane i
      6|     7     |8
      ---------------                k
       |           |               ^ 
       |           |               |
      5|    myid   |1              |
       |           |              ------>
       |           |               |     j
      ---------------
      4|     3     |2

 plane i-1                  plane i+1
     15|    16     |17          24|    25     |26
      ---------------            --------------- 
       |           |              |           |
       |           |              |           |
     14|     9     |10          23|    18     |19
       |           |              |           |
       |           |              |           |
      ---------------            ---------------
     13|    12     |11          22|    21     |20


       = 0 if boundary between
       |   subdomain of process
       |   myid and subdomain of
       |   process neighbor(i)
 bd(i)=|   is inside the domain
       |   or is a limit of the
       |   domain with wall BC
       |
       |
       = 1 if boundary between
           subdomain of process
           myid and subdomain of
           process neighbor(i)
           is a wall.

- the datatype to represent a real number (realtype). All other
  necessary datatypes are defined in mgdinit.

In the test program, these variables are defined in the main
program. In addition, before solving for anything, the calling
program must first execute mgdinit in order to initialize the
indices and the datatypes at the different grid levels. The
arguments of mgdinit are:
inputs
------
- vbc : the values at the 4 boundaries when Dirichlet boundary
  conditions are used
- ixp,jyq,kzr,iex,jey,kez,ngrid,nxp2,nyp2,nzp2 : the resolution
  of the global domain, such that
    nxp2-2=ixp*2**(iex-1)
    nyp2-2=jyq*2**(jey-1)
    nzp2-2=kzr*2**(kez-1)
    ngrid=max(iex,jey,kez)
- sx,ex,sy,ey,sz,ez : the starting and ending indices of the
  local subdomain
- realtype: datatype for a real number
- nxprocs,nyprocs,nzprocs : the number of subdomains/processes
  in the i-, j-, and k-directions (the total number of
  processes is numprocs=nxprocx*nyprocs*nzprocs)
- nwork : the size of the work vector; I use the following formula
  to calculate a value that is satisfying for most cases
       nwork=9*int(8.0/7.0*float(nx*ny*nz)
                           /float(nxprocs*nyprocs*nzprocs))
                     +27*int(4.0/3.0*(
                         float(nx*ny)/float(nxprocs*nyprocs)
                        +float(nx*nz)/float(nxprocs*nzprocs)
                        +float(ny*nz)/float(nyprocs*nzprocs)))
                     +81*int(float(nx)/float(nxprocs)
                            +float(ny)/float(nyprocs)
                            +float(nz)/float(nzprocs))
                     +243*ngrid
- ibdry,jbdry,kbdry : boundary conditions in x, y, and z
     0 -> periodic
     1 -> Neumann
     2 -> Dirichlet
- myid : the rank of the process which supports the subdomain
  determined by sx,ex,sy,ey,sz,ez
- IOUT : integer number that is the unit to which all printouts
  are directed
outputs
-------
- phibc : array used to transfer the values of vbc to the
  different grid levels
- nerror : an error flag which is 0 if everything iss fine and
  1 if an error occurred during the initialization; the code
  does not abort, this is left up to the calling program

The mgd commmon passes all the information that is needed by
the rest of the code: indices of the subdomain at the different
grid levels, indices of the different arrays in the work vector,
and datatypes to exchange planes, lines, and corners of boundary
data between neighboring subdomains at the different grid levels.

Once mgdinit has executed, it is possible to solve as many
Poisson equation problems as needed, as long as the underlying
grid remains the same. The calling program must call the
mgdsolver subroutine with the following arguments:
inputs
------
- isol = 1 : solve the simpler, separable, equation
              d(d(phi)/dx)/dx+d(d(phi)/dy)/dy+d(d(phi)/dz)=rhs,
             in other words, the coefficients cof are all 1;
             this is used in my multiphase fluid code to solve
             for the density
       = 2 : solve the complete equation with the coefficients
             as written at the top being the reciprocal of the
             density; I use that option to solve for the pressure
             in my problem
- sx,ex,sy,ey,sz,ez : indices of the subdomain
- phif : an array of dimensions at least (sx-1:ex+1,sy-1:ey+1,
         sz-1:ez+1),contains an initial approximation of phi at
         the finest level, set to zero if nothing better
- rhsf : an array of dimensions at least (sx-1:ex+1,sy-1:ey+1,
         sz-1:ez+1), contains the rhs at the finest grid level 
         of the Poisson equation
- r : an array of dimensions at least (sx-1:ex+1,sy-1:ey+1,
      sz-1:ez+1), contains the reciprocal of the values of the
      coefficients at the same locations as the phif. If isol=1,
      nothing is done with r. If isol=2, the values of r are
      used to calculate the coefficients cof, e.g.
        cof(i+1/2,j,k)=2/(r(i,j,k)+r(i+1,j,k))
- ngrid : maximum number of grid levels, defined as previously
- work : a work space of length nwork
- maxcy : the maximum number of multigrid cycles (I set it to
          300, but it usually converges to a correct accuracy
          in 2 to 10 cycles, so hitting the maxcy limit 
          indicates a critical or ill-conditioned problem)
- tolmax : desired accuracy (I set it to 1.0e-6)
- kcycle = 1 : use V-cycles
           2 : use W-cycles
           (I use 1)
- iprer : number of relaxation sweeps executed before 
          restricting the residual to the next coarser level
          (I set it to 2)
- ipost : number of relaxation sweeps executed after adding
          the residual correction from the next coarser
          level to the current level
          (I set it to 1)
- iresw = 1 : use fully weighted residuals
        = 2 : use half-weighted residuals
          (I advise using full weighting for robustness)
- xl,yl,zl : physical limits of the global domain
- rro : average value of r. Since the solution of the elliptic
        equation is modulo a constant, this value serves to set
        the average value of the solution when isol=1; when
        isol=0, the average value is set to 0.
- nx,ny,nz : nx=nxp2-2, ny=nyp2-2, nz=nzp2-2
- comm3d,comm3dp,comm3dl,comm3dc : MPI communicators
- myid : rank of the process
- neighbor : ranks of the neighbors
- bd : boundary conditions at the boundaries with the 
       neighboring subdomains
- phibc : for Dirichlet boundary conditions, gives the values
          at the boundaries for the different grid levels
- nprscr = true : print out messages
         = false : do not print out messages
- IOUT : unit on which to direct the printouts
outputs
-------
- phif : the approximate solution to the discretization
         of the partial differential equation
- iter : number of multigrid cycles used to reach the desired
         accuracy
- nerror = 0 : no error
           1 : an error occurred
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+
+  NOTES
+
1). The grid at the coarsest level must have at least one
    point in each subdomain, i.e. ixp.ge.nxprocs, 
    jyq.ge.nyprocs, & kzr.ge.nzprocs. The code works for any
    combination of (ixp,iex), (jyq,jey), and (kzr,kez) that
    satisfies these conditions (along with the relations
    between these parameters and nxp2, nyp2, nzp2). In
    particular, it is possible to have iex.ne.jey.ne.kez,
    which means that no coarsifying will take place in some
    direction between some levels.
2). At the finest grid level, we must have either
      iex.gt.1 AND jey.gt.1 AND kez.gt.1
    or
      iex=1 AND jey=1 AND kez=1.
    The first condition means that, if there is coarsifying
    at the finest grid level, it must must take place in all
    spatial directions; the reason for this restriction has
    to do with programming convenience, not the multigrid
    method itself. The second condition means that the code
    can be used to do simple Gauss-Seidel on a single grid
    level; this is helpful for tests.
3). Experience has shown that the best perfomance is achieved 
    when each subdomain has 2 points in each direction, i.e.
           ixp=2*nxprocs, jyq=2*nyprocs, kzr=2*nzprocs.
4). If one has finished using the multigrid code and wants to
    deallocate the MPI variables attached to it properly, one
    can call the subroutine 'mgdend' with the argument 'ngrid'.
    This frees up the MPI datatypes used in the code and 
    initialized in 'mgdinit'.
